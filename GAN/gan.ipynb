{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efc2c266-b95f-4fd3-a961-25ede55a33c5",
   "metadata": {},
   "source": [
    "# GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35254ca0-ce10-4ec2-9db4-8f75145b516e",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6621ee-ee05-4b5d-a5bc-46defc1d39c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.layers as tfl\n",
    "import tensorflow.keras.models as tfm\n",
    "from tensorflow.keras.optimizers.legacy import Adam \n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from progressbar import progressbar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5effae-ac80-49d7-bd05-036c3fe32f3a",
   "metadata": {},
   "source": [
    "## Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7131f87c-f72b-4b11-b0b5-7b561a030671",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL2CLASS = {0: 'T-shirt/top', \n",
    "               1: 'Trouser',\n",
    "               2: 'Pullover',\n",
    "               3: 'Dress',\n",
    "               4: 'Coat',\n",
    "               5: 'Sandal',\n",
    "               6: 'Shirt',\n",
    "               7: 'Sneaker',\n",
    "               8: 'Bag',\n",
    "               9: 'Ankle Boot'}\n",
    "HEIGHT = 28\n",
    "WIDTH = 28\n",
    "CHANNEL = 1\n",
    "BATCH_SIZE = 128\n",
    "LATENT_DIM = 128\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5e2403-27e0-402c-afde-aa4093d09d2a",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cdeaa7-022a-4682-93bd-9bfdeee01d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "x_train\n",
    "# gather together all the images\n",
    "x = np.concatenate([x_train, x_test], 0) ; y = np.concatenate([y_train, y_test], 0)\n",
    "\n",
    "# add channel dimension\n",
    "x = np.expand_dims(x, axis = -1)\n",
    "\n",
    "# scale data\n",
    "x = x.astype('float64')\n",
    "y = y.astype('uint8')\n",
    "x /= 255.0\n",
    "\n",
    "# make x and y to batches\n",
    "# x: (number of batches, batch size, height, width, channel)\n",
    "# y: (number of batches, batch size, 1)\n",
    "num_of_pics = x.shape[0] ; num_of_batches = int(num_of_pics / BATCH_SIZE)\n",
    "x_batch = np.zeros((num_of_batches, BATCH_SIZE, HEIGHT, WIDTH, CHANNEL), dtype = np.float64)\n",
    "y_batch = np.zeros((num_of_batches, BATCH_SIZE), dtype = np.uint8)\n",
    "for i in range(num_of_batches):\n",
    "    x_batch[i] = x[i * BATCH_SIZE : (i + 1) * BATCH_SIZE]\n",
    "    y_batch[i] = y[i * BATCH_SIZE : (i + 1) * BATCH_SIZE]\n",
    "\n",
    "\n",
    "# convert to tensor\n",
    "x = tf.convert_to_tensor(x)\n",
    "x_batch = tf.convert_to_tensor(x_batch)\n",
    "y = tf.convert_to_tensor(y)\n",
    "y_batch = tf.convert_to_tensor(y_batch)\n",
    "\n",
    "print(f\"x_batch shape: {x_batch.shape}\") \n",
    "print(f\"y_batch shape: {y_batch.shape}\")\n",
    "print(f\"x shape: {x.shape}\") \n",
    "print(f\"y shape: {y.shape}\")\n",
    "\n",
    "print(f\"x_batch dtype: {x_batch.dtype}\") \n",
    "print(f\"y_batch dtype: {y_batch.dtype}\")\n",
    "print(f\"x dtype: {x.dtype}\") \n",
    "print(f\"y dtype: {y.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3829c07-dd2b-47d4-ab64-f818334f9786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(img, title):\n",
    "    plt.imshow(img, 'gray')\n",
    "    plt.title(title)\n",
    "\n",
    "idx = 100\n",
    "show_image(x[idx], LABEL2CLASS[int(y[idx])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568563a9-48fc-4ab4-858a-ac9c29bf5a37",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af684c2c-1439-4d8e-8750-3492098747b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generator(input_dim):\n",
    "\n",
    "    # input\n",
    "    input = tf.keras.Input(input_dim)\n",
    "\n",
    "    # from input_dim -> 7x7xinput_dim\n",
    "    x = tfl.Dense(units = 7 * 7 * input_dim)(input)\n",
    "    x = tfl.LeakyReLU(0.2)(x)\n",
    "    x = tfl.Reshape((7, 7, input_dim))(x)\n",
    "\n",
    "    # upsampling\n",
    "    # from 7x7xinput_dim -> 14x14x128\n",
    "    x = tfl.UpSampling2D()(x)\n",
    "    x = tfl.Conv2D(128, 5, padding = 'same')(x)\n",
    "    x = tfl.LeakyReLU(0.2)(x)\n",
    "\n",
    "    # upsampling\n",
    "    # from 14x14x128 -> 28x28x128\n",
    "    x = tfl.UpSampling2D()(x)\n",
    "    x = tfl.Conv2D(128, 5, padding = 'same')(x)\n",
    "    x = tfl.LeakyReLU(0.2)(x)\n",
    "\n",
    "    # conv\n",
    "    x = tfl.Conv2D(128, 4, padding='same')(x)\n",
    "    x = tfl.LeakyReLU(0.2)(x)\n",
    "    x = tfl.Conv2D(128, 4, padding='same')(x)\n",
    "    x = tfl.LeakyReLU(0.2)(x)\n",
    "    x = tfl.Conv2D(1, 4, padding = 'same', activation = 'sigmoid')(x)\n",
    "\n",
    "    # output\n",
    "    output = x\n",
    "    \n",
    "    model = tfm.Model(inputs = input, outputs = output)\n",
    "    \n",
    "    return model\n",
    "    \n",
    "gen = get_generator(input_dim = LATENT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4143ee56-a5b5-40bb-b806-3593fc3cf200",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ada16a-2176-4e5e-8ba2-865c5df0e12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a random image\n",
    "random_numbers = np.random.rand(10, LATENT_DIM, CHANNEL)\n",
    "random_image = gen(random_numbers)\n",
    "random_image = random_image[5]\n",
    "show_image(random_image, 'random image')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb179477-0184-415e-b421-d246962f55a1",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c45cb52-3c3e-471b-8293-eea4bb466e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discriminator(input_dim):\n",
    "    \n",
    "    input = tf.keras.Input(input_dim)\n",
    "\n",
    "    x = tfl.Conv2D(32, 5, padding = 'valid')(input)\n",
    "    x = tfl.LeakyReLU(0.2)(x)\n",
    "    x = tfl.Dropout(0.4)(x)\n",
    "\n",
    "    x = tfl.Conv2D(64, 5, padding = 'valid')(x)\n",
    "    x = tfl.LeakyReLU(0.2)(x)\n",
    "    x = tfl.Dropout(0.4)(x)\n",
    "\n",
    "\n",
    "    x = tfl.Conv2D(128, 5, padding = 'valid')(x)\n",
    "    x = tfl.LeakyReLU(0.2)(x)\n",
    "    x = tfl.Dropout(0.4)(x)\n",
    "\n",
    "    x = tfl.Conv2D(256, 5, padding = 'valid')(x)\n",
    "    x = tfl.LeakyReLU(0.2)(x)\n",
    "    x = tfl.Dropout(0.4)(x)\n",
    "\n",
    "    x = tfl.Flatten()(x)\n",
    "    x = tfl.Dropout(0.4)(x)\n",
    "    \n",
    "    output = tfl.Dense(units = 1, activation = 'sigmoid')(x)\n",
    "    \n",
    "    model = tfm.Model(inputs = input, outputs = output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464d7e77-aac1-4f97-92a8-742cb3d1b6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dis = get_discriminator((HEIGHT, WIDTH, CHANNEL))\n",
    "dis.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3065539d-782d-4935-b0e6-1474ba2d4250",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_image = tf.expand_dims(random_image, axis = 0)\n",
    "dis(random_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28329370-3f11-4984-9e3d-b55918b0bc69",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164bfb2b-924b-4dc2-8512-f6d525d732bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# real image -> 0\n",
    "# fake image -> 1\n",
    "class GAN(tfm.Model):\n",
    "    def __init__(self, generator, discriminator, latent_dim, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.gen = generator\n",
    "        self.dis = discriminator\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def compile(self, g_opt, d_opt, g_loss, d_loss, *args, **kwargs): \n",
    "        super().compile(*args, **kwargs)\n",
    "        self.g_opt = g_opt\n",
    "        self.d_opt = d_opt\n",
    "        self.g_loss = g_loss\n",
    "        self.d_loss = d_loss \n",
    "\n",
    "    def train_step(self, image_batch):\n",
    "        batch_size, height, width, channel = image_batch.shape\n",
    "        \n",
    "        # get real and fake images\n",
    "        real_images = image_batch\n",
    "        fake_images = self.gen(tf.random.normal((batch_size, self.latent_dim, 1)),  training=False)\n",
    "\n",
    "        # train dis\n",
    "        with tf.GradientTape() as d_tape:\n",
    "\n",
    "            # make predictions\n",
    "            yhat_real = self.dis(real_images, training = True)\n",
    "            yhat_fake = self.dis(fake_images, training = True)\n",
    "            yhat = tf.concat([yhat_real, yhat_fake], axis = 0)\n",
    "\n",
    "            # correct labels\n",
    "            y = tf.concat([tf.zeros_like(yhat_real), tf.ones_like(yhat_fake)], axis = 0)\n",
    "\n",
    "            # Add some noise to the correct labels\n",
    "            noise_real = 0.15*tf.random.uniform(tf.shape(yhat_real))\n",
    "            noise_fake = -0.15*tf.random.uniform(tf.shape(yhat_fake))\n",
    "            y += tf.concat([noise_real, noise_fake], axis=0)\n",
    "            \n",
    "            # Calculate loss - BINARYCROSS \n",
    "            dis_loss = self.d_loss(y, yhat)\n",
    "\n",
    "        # apply gradient\n",
    "        dgrad = d_tape .gradient(dis_loss, self.dis.trainable_variables)\n",
    "        self.d_opt.apply_gradients(zip(dgrad, self.dis.trainable_variables))\n",
    "\n",
    "        # train gan\n",
    "        with tf.GradientTape() as g_tape:\n",
    "            # generate fake images\n",
    "            fake_images = self.gen(tf.random.normal((batch_size, self.latent_dim, 1)),  training=True)\n",
    "\n",
    "            # get predicted labels\n",
    "            yhat = self.dis(fake_images, training = False)\n",
    "\n",
    "            # wished labels, we want fake images to be real images\n",
    "            wished_labels = tf.zeros_like(yhat)\n",
    "\n",
    "            gen_loss = self.g_loss(yhat, wished_labels)\n",
    "\n",
    "        # apply gradient\n",
    "        ggrad = g_tape .gradient(gen_loss, self.gen.trainable_variables)\n",
    "        self.g_opt.apply_gradients(zip(ggrad, self.gen.trainable_variables))\n",
    "        \n",
    "        return {\"dis_loss\": dis_loss, \"gen_loss\": gen_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26419738-7613-4b74-9ae4-e8b3b1c291ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator and discriminator\n",
    "gen = get_generator(input_dim = LATENT_DIM)\n",
    "dis = get_discriminator((HEIGHT, WIDTH, CHANNEL))\n",
    "\n",
    "# define loss \n",
    "g_loss = BinaryCrossentropy()\n",
    "d_loss = BinaryCrossentropy()\n",
    "\n",
    "# define optimizers\n",
    "g_opt = Adam(learning_rate = 0.0001)\n",
    "d_opt = Adam(learning_rate = 0.00001)\n",
    "\n",
    "gan_model = GAN(gen, dis, LATENT_DIM)\n",
    "gan_model.compile(g_opt, d_opt, g_loss, d_loss)\n",
    "\n",
    "d_losses = []\n",
    "g_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    for batch_idx in progressbar( range(num_of_batches) ):\n",
    "        losses = gan_model.train_step(x_batch[batch_idx])\n",
    "        d_losses.append(losses['dis_loss'])\n",
    "        g_losses.append(losses['gen_loss'])\n",
    "        if batch_idx == (num_of_batches - 1):\n",
    "            print(f\"Discriminator Loss: {losses['dis_loss']} --- Generator Loss: {losses['gen_loss']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8fd315-78a1-4466-9909-f1d5d91dafee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
